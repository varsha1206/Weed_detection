# -*- coding: utf-8 -*-
"""Copy of Untitled53.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ZWIk-xAPhsPT4q4KQQRgAMnrWbt6-Q1
"""



import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers

import matplotlib.pyplot as plt

from google.colab import drive

#DATASET_DIR = '/content/drive/MyDrive/MEDICINENEW'
train_path             = "/content/drive/MyDrive/MINIPROJECT_Dataset/TRAIN"
test_path              = "/content/drive/MyDrive/MINIPROJECT_Dataset/TEST"



from skimage import data, exposure, img_as_float

def AHE(x_train):
  img_adapteg=exposure.equalize_adapthist(x_train,clip_limit=0.03)
  return img_adapteg

train_image_generator = ImageDataGenerator(
    rotation_range=25, width_shift_range=0.1,
    height_shift_range=0.1, shear_range=0.2,  
    horizontal_flip=True, 
    fill_mode="nearest",
    #preprocessing_function=AHE,
    rescale = 1.0/255)
test_image_generator = ImageDataGenerator(
    rescale = 1.0/255)

training_iterator=[]

training_iterator = train_image_generator.flow_from_directory('/content/drive/MyDrive/MINIPROJECT_Dataset/train',
target_size = (224, 224),
batch_size = 16)
#color_mode = 'grayscale')
#class_mode = 'sparse')

print(type(training_iterator))



# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import cv2
import random
import numpy as np
from tqdm import tqdm
import pickle
from keras.models import Sequential 

from keras.layers import Conv2D, MaxPooling2D 
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
# %matplotlib inline

import os
train_images_data = []
path2="/content/drive/MyDrive/MINIPROJECT_Dataset/train/Black_nightshade"
path1="/content/drive/MyDrive/MINIPROJECT_Dataset/train/Tomato"
image_files = os.listdir(path2)


  
#random.shuffle(training_iterator)

for image in tqdm(image_files):
    image_data = cv2.imread(path2+'/'+image)
    
    #Convert to GrayScale
    gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
    image_data = cv2.resize(gray, (64, 64))
    #convert color from BGR to RGB
    #image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
    
    
    val,th = cv2.threshold(gray, 127,255,cv2.THRESH_BINARY)
 # find contours
    contours,_ = cv2.findContours(th, 
                               cv2.RETR_TREE,
                               cv2.CHAIN_APPROX_NONE)
 # draw contours on original image
 # arguments: image, contours list, index/content/drive/MyDrive/MEDICINENEW/train of contour, colour, thickness
    cv2.drawContours(image_data, contours, -1, (0,0,255),1)
    #turn to only borders
    image_data = cv2.Canny(image_data, 150, 150)
    
    train_images_data.append(image_data)
    train_images_data.append(image_data[:, ::-1]) #flipped image

import os
path2="/content/drive/MyDrive/MINIPROJECT_Dataset/train/Black_nightshade"
path1="/content/drive/MyDrive/MINIPROJECT_Dataset/train/Tomato"
imgf = os.listdir(path1)


  
#random.shuffle(training_iterator)

for image in tqdm(imgf):
    image_data = cv2.imread(path1+'/'+image)
    
    #Convert to GrayScale
    gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
    image_data = cv2.resize(gray, (64, 64))
    #convert color from BGR to RGB
    #image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)
    
    
    val,th = cv2.threshold(gray, 127,255,cv2.THRESH_BINARY)
 # find contours
    contours,_ = cv2.findContours(th, 
                               cv2.RETR_TREE,
                               cv2.CHAIN_APPROX_NONE)
 # draw contours on original image
 # arguments: image, contours list, index/content/drive/MyDrive/MEDICINENEW/train of contour, colour, thickness
    cv2.drawContours(image_data, contours, -1, (0,0,255),1)
    #turn to only borders
    image_data = cv2.Canny(image_data, 150, 150)
    
    train_images_data.append(image_data)
    train_images_data.append(image_data[:, ::-1]) #flipped image

test_iterator = test_image_generator.flow_from_directory('/content/drive/MyDrive/MINIPROJECT_Dataset/test',
target_size = (224, 224),
batch_size = 16)
#color_mode = 'grayscale')
#class_mode = 'sparse')

import keras
import keras.backend as K
from keras.models import Model
from keras.layers.core import Dense,Dropout,Activation,Flatten,Lambda

from keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose
from keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization
from keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation

from keras.layers import LeakyReLU

from keras.preprocessing.image import ImageDataGenerator
from keras.applications import densenet
from keras.models import Sequential, Model, load_model
from keras.layers import Conv2D, MaxPooling2D
#from keras.layers.convolutional import AtrousConvolution2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from keras import regularizers
from keras import backend as K

"""**Inception**"""

from keras.preprocessing.image import ImageDataGenerator
from keras.applications import densenet
from keras.models import Sequential, Model, load_model
from keras.layers import Conv2D, MaxPooling2D
#from keras.layers.convolutional import AtrousConvolution2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback
from keras import regularizers
from keras import backend as K

IMAGE_SIZE = [224, 224]

from tensorflow.keras.applications.inception_v3 import InceptionV3

#from keras.layers import Activation, DenseNet121
inception = tf.keras.applications.densenet.DenseNet121(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

for layer in inception.layers:
    layer.trainable = False

x = Flatten()(inception.output)
prediction = Dense(1, activation='sigmoid')(x)

model = Model(inputs=inception.input, outputs=prediction)

# View the structure of the model
model.summary()

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=["accuracy"])

model.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['acc'])

#model.compile(
#optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),
#loss = tf.keras.losses.binary_crossentropy(),
#metrics = ['accuracy'])

model.summary()

hist = model.fit(x_train,y_train,batch_size=32,
          epochs=30,verbose=1,validation_split=0.2,shuffle=True)





import tensorflow as tf
from tensorflow import keras

import pandas as pd
import numpy as np
import os
import keras
import random
import cv2
import math
import seaborn as sns

from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization
from tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.densenet import preprocess_input

from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array

from tensorflow.keras.models import Model

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau

import warnings
warnings.filterwarnings("ignore")

from tensorflow.keras.applications.densenet import DenseNet121

model_d=DenseNet121(weights='imagenet',include_top=False, input_shape=(128, 128, 3))



x=model_d.output

x= GlobalAveragePooling2D()(x)
x= BatchNormalization()(x)
#x= Dropout(0.5)(x)
x= Dense(1,activation='relu')(x) 
x= Dense(1,activation='relu')(x) 
x= BatchNormalization()(x)
#x= Dropout(0.5)(x)

preds=Dense(1,activation='softmax')(x) #FC-layer

from google.colab import drive
drive.mount('/content/drive')

model=Model(inputs=x.input,outputs=preds)
model.summary()





# model = Sequential()
# model.add(layers.Input(shape = training_iterator.image_shape))
# model.add(layers.Conv2D(16, 3, padding ='same', activation = 'relu'))
# #model.add(layers.Dropout(0.1))
# model.add(layers.MaxPooling2D())
# #model.add(layers.Dropout(0.2))
# model.add(layers.Conv2D(32, 3, padding = 'same', activation = 'relu'))
# model.add(layers.MaxPooling2D())
# model.add(layers.Flatten())
# #model.add(layers.Dense(128,activation = 'relu'))
# model.add(layers.Dense(2, activation = 'softmax'))

# model.compile(
# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
# loss = tf.keras.losses.CategoricalCrossentropy(),
# metrics = ['accuracy'])

# model.summary()

model.compile(
optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
loss = tf.keras.losses.BinaryCrossentropy(),
metrics = ['accuracy'])

model.summary()

batch_size = 16

model = Sequential()
model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
model.add(Conv2D(filters=256, kernel_size=(1,1), padding="same", activation="relu"))
# model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Flatten())
model.add(Dense(units=128,activation="relu"))
model.add(Dense(units=128,activation="relu"))
model.add(Dense(units=1, activation="softmax"))

# model.compile(
# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
# loss = tf.keras.losses.CategoricalCrossentropy(),
# metrics = ['accuracy'])
model.compile(optimizer='adam',
loss='binary_crossentropy',
metrics=['acc'])
model.summary()









history = model.fit(training_iterator,epochs=5,validation_data = test_iterator)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn
import pandas as pd
import pickle
import csv

from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from PIL import Image

import os

from sklearn.metrics import mean_squared_error
plt.figure(0)
plt.plot(history.history['accuracy'],'r')
plt.plot(history.history['val_accuracy'],'g')
plt.xticks(np.arange(0, 20, 1.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Accuracy")
plt.title("Training Accuracy vs Validation Accuracy")
plt.legend(['train','validation'])
 
plt.figure(1)
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'g')
plt.xticks(np.arange(0, 20, 1.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Loss")
plt.title("Training Loss vs Validation Loss")
plt.legend(['train','validation'])

plt.figure(2)
plt.plot(history.history['mean_squared_error'],'r')
plt.plot(history.history['val_mean_squared_error'],'g')
plt.xticks(np.arange(0, 20, 1.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("MSE")
plt.title("Training Loss vs Validation Loss")
plt.legend(['train','validation'])
 
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sn
import pandas as pd
import pickle
import csv

from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from PIL import Image

import os



